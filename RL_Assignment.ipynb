{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "191RftiGagEr4N99OY3ZBnJJ-19WJeEse",
      "authorship_tag": "ABX9TyPv1EdhLOIMGNurQKn+5/4h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amogh-code2021/Tutorials_1/blob/master/RL_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size = 5>**Implementation based RL Assignment**</font>\n",
        "\n",
        "I will be using the openAI gym environment, specifically CartPole-v1 for this porject. I will be using Q learning algorithm (greedy with epsilon) to arrive at a Final reward. Additionally I will also be using Deep Q Networks for this project with different Q learning algorithms to compare results. I couldn't include the render unfortunately as render does not work on Google Colab."
      ],
      "metadata": {
        "id": "UwED9mZdvZJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin with installing and importing all necessary packages for the project"
      ],
      "metadata": {
        "id": "kUDqXymYvYxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras\n",
        "!pip install keras-rl2"
      ],
      "metadata": {
        "id": "kYTV_XbFhY8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pfikcGqkgZsj"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import time # to get the time\n",
        "import math # needed for calculations\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from rl.agents import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, GreedyQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "metadata": {
        "id": "kStujMx5hSQv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size = 4>**Setting up cartPole environment**</font> \n",
        "\n",
        "It is one of the classic Open AI environments. Here a cart is trying to balance the pole vertically.\n",
        "The only movement allowed is left or right (-1 or +1). If the cart position moves by more than +2.4 or -2.4, pole angle is more that +/- 12 degrees and episode length is more than 500 the episode ends [2]"
      ],
      "metadata": {
        "id": "UhIOB5GFwhjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"CartPole-v1\"\n",
        "env = gym.make(env_name)"
      ],
      "metadata": {
        "id": "u03AYuCqNpHy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The “Observation” variable is slightly unique, however. The reason that the array was manually set was that the first two variables (Cart position, Cart Velocity) is not as important as the other two, (Pole Angle, Pole Velocity)[1].\n",
        "\n",
        "The np_array_win_size is the “steps” based upon cart position, cart velocity, pole angle, and then pole velocity.[1]"
      ],
      "metadata": {
        "id": "D2H1VlhDwoI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Observation = [30, 30, 50, 50]\n",
        "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
        "\n",
        "\n",
        "#Get state values in discretised format\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = state/np_array_win_size+ np.array([15,10,1,10])\n",
        "    return tuple(discrete_state.astype(np.int))"
      ],
      "metadata": {
        "id": "lKe7NuEDNu-H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size = 4>**Q - Learning**</font>\n",
        "\n",
        "Function to train the q model. Here initial epsilon value of 1, learning rate of 0.1, discount rate of 0.95 and 70000 episodes were chosen as default values.\n"
      ],
      "metadata": {
        "id": "ulht0sjizDbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
        "\n",
        "def q_train_1(env, q_table, eps = 1.0, epsilon_decay_value = 0.99995, lr = 0.1, disc_rate = 0.95, episodes = 70000):\n",
        "    \n",
        "    #Setting up initial values\n",
        "    total_reward = 0\n",
        "    prior_reward = 0\n",
        "    #The episode rewards starts from 0 before every episode loop\n",
        "    episode_reward = 0\n",
        "\n",
        "    #Running loop over all episodes\n",
        "    for episode in range(episodes + 1):\n",
        "    \n",
        "        state = env.reset()\n",
        "        discrete_state = get_discrete_state(state)\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            #Taking epsiln into consideration, moves the model towards coordinated action\n",
        "            if np.random.random() > eps:\n",
        "                action = np.argmax(q_table[discrete_state])\n",
        "            else:\n",
        "              #Random action (These are mostly taken intially until the algorithm starts learning)\n",
        "                action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "            #Get new state\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            episode_reward += reward\n",
        "\n",
        "            new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "            if not done:\n",
        "\n",
        "                #Updating Q table\n",
        "                future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "                current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "                new_q = (1 - lr) * current_q + lr * (reward + disc_rate * future_q)\n",
        "\n",
        "                q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "            discrete_state = new_discrete_state\n",
        "        \n",
        "        if eps > 0.05: #Updating epsilon value. Updated every 10000 episodes\n",
        "            if episode_reward > prior_reward and episode > 10000:\n",
        "                eps = math.pow(epsilon_decay_value, episode - 10000)\n",
        "\n",
        "        total_reward += episode_reward #Total reward of the episode\n",
        "        prior_reward = episode_reward\n",
        "\n",
        "        if episode % 1000 == 0: #every 1000 episodes print the average time and the average reward\n",
        "\n",
        "            mean_reward = total_reward / 1000\n",
        "            print(\"Episode: \" + str(episode) + \" Mean Reward: \" + str(mean_reward))\n",
        "\n",
        "            total_reward = 0"
      ],
      "metadata": {
        "id": "hcOeB_V9Nyib"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adter training the model, we can see that initally the rewards were not increasing by much (till episode 10000). As soon as the algorithm learns we can see that there is a dramatic increase in reward there on. After 50000 episodes we can see that a mean reward of above 200 was obtained."
      ],
      "metadata": {
        "id": "DVoE1A6N1_oK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_train_1(env,q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVtWEHZJN1zj",
        "outputId": "29eaa1f1-a7a1-4668-d4f1-0197623aaf2b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0 Mean Reward: 0.021\n",
            "Episode: 1000 Mean Reward: 22.351\n",
            "Episode: 2000 Mean Reward: 21.631\n",
            "Episode: 3000 Mean Reward: 22.502\n",
            "Episode: 4000 Mean Reward: 21.949\n",
            "Episode: 5000 Mean Reward: 21.279\n",
            "Episode: 6000 Mean Reward: 22.259\n",
            "Episode: 7000 Mean Reward: 21.853\n",
            "Episode: 8000 Mean Reward: 21.866\n",
            "Episode: 9000 Mean Reward: 21.811\n",
            "Episode: 10000 Mean Reward: 21.909\n",
            "Episode: 11000 Mean Reward: 22.804\n",
            "Episode: 12000 Mean Reward: 23.642\n",
            "Episode: 13000 Mean Reward: 25.102\n",
            "Episode: 14000 Mean Reward: 26.625\n",
            "Episode: 15000 Mean Reward: 28.616\n",
            "Episode: 16000 Mean Reward: 30.389\n",
            "Episode: 17000 Mean Reward: 34.879\n",
            "Episode: 18000 Mean Reward: 35.107\n",
            "Episode: 19000 Mean Reward: 39.091\n",
            "Episode: 20000 Mean Reward: 43.785\n",
            "Episode: 21000 Mean Reward: 43.877\n",
            "Episode: 22000 Mean Reward: 48.849\n",
            "Episode: 23000 Mean Reward: 54.363\n",
            "Episode: 24000 Mean Reward: 58.403\n",
            "Episode: 25000 Mean Reward: 63.629\n",
            "Episode: 26000 Mean Reward: 68.394\n",
            "Episode: 27000 Mean Reward: 73.323\n",
            "Episode: 28000 Mean Reward: 76.632\n",
            "Episode: 29000 Mean Reward: 84.506\n",
            "Episode: 30000 Mean Reward: 86.817\n",
            "Episode: 31000 Mean Reward: 92.291\n",
            "Episode: 32000 Mean Reward: 104.41\n",
            "Episode: 33000 Mean Reward: 107.682\n",
            "Episode: 34000 Mean Reward: 115.232\n",
            "Episode: 35000 Mean Reward: 118.034\n",
            "Episode: 36000 Mean Reward: 124.505\n",
            "Episode: 37000 Mean Reward: 132.943\n",
            "Episode: 38000 Mean Reward: 137.202\n",
            "Episode: 39000 Mean Reward: 139.254\n",
            "Episode: 40000 Mean Reward: 152.66\n",
            "Episode: 41000 Mean Reward: 170.906\n",
            "Episode: 42000 Mean Reward: 176.939\n",
            "Episode: 43000 Mean Reward: 164.606\n",
            "Episode: 44000 Mean Reward: 165.453\n",
            "Episode: 45000 Mean Reward: 179.122\n",
            "Episode: 46000 Mean Reward: 171.695\n",
            "Episode: 47000 Mean Reward: 199.005\n",
            "Episode: 48000 Mean Reward: 214.208\n",
            "Episode: 49000 Mean Reward: 178.601\n",
            "Episode: 50000 Mean Reward: 186.048\n",
            "Episode: 51000 Mean Reward: 207.871\n",
            "Episode: 52000 Mean Reward: 225.191\n",
            "Episode: 53000 Mean Reward: 193.447\n",
            "Episode: 54000 Mean Reward: 221.54\n",
            "Episode: 55000 Mean Reward: 240.85\n",
            "Episode: 56000 Mean Reward: 242.948\n",
            "Episode: 57000 Mean Reward: 246.788\n",
            "Episode: 58000 Mean Reward: 218.386\n",
            "Episode: 59000 Mean Reward: 224.721\n",
            "Episode: 60000 Mean Reward: 216.343\n",
            "Episode: 61000 Mean Reward: 245.026\n",
            "Episode: 62000 Mean Reward: 184.152\n",
            "Episode: 63000 Mean Reward: 214.497\n",
            "Episode: 64000 Mean Reward: 206.619\n",
            "Episode: 65000 Mean Reward: 197.54\n",
            "Episode: 66000 Mean Reward: 196.04\n",
            "Episode: 67000 Mean Reward: 186.645\n",
            "Episode: 68000 Mean Reward: 180.484\n",
            "Episode: 69000 Mean Reward: 195.5\n",
            "Episode: 70000 Mean Reward: 188.876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size = 4>**Deep Q learning Networks**</font>\n",
        "\n",
        "Here I implement combination of a Neural networks with multiple Q learning algorithms. The Neural Network remains the same for all Q learning algorithms [3]. For the project I have picked Boltzmann Q-policy,\n",
        "Epsilon greedy Q-policy and greedy Q policy."
      ],
      "metadata": {
        "id": "ajCxE5Rh2nqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to build NN and Agent"
      ],
      "metadata": {
        "id": "wi6Ocrb04IG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(states, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1,states)))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model\n",
        "\n",
        "def build_agent(model, actions, policy = BoltzmannQPolicy()):\n",
        "    memory = SequentialMemory(limit=50000, window_length=1)\n",
        "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
        "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
        "    return dqn\n"
      ],
      "metadata": {
        "id": "b1MmsMb4VkWz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states = env.observation_space.shape[0]\n",
        "actions = env.action_space.n"
      ],
      "metadata": {
        "id": "0Eh9OrQWWDM4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(states, actions)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-oYTCgsVsRg",
        "outputId": "7d386b28-a70c-4804-ffe4-bf222e8acbe3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 4)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 24)                120       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 24)                600       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 50        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 770\n",
            "Trainable params: 770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build and train DQN. Number of steps here are 50000"
      ],
      "metadata": {
        "id": "Xw0S4gKq4cBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = build_agent(model, actions, policy= EpsGreedyQPolicy())\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mYM8JhOWj0D",
        "outputId": "0af3696d-44ba-4d44-dc49-ab7b26518498"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r    1/10000 [..............................] - ETA: 50:15 - reward: 1.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 77s 8ms/step - reward: 1.0000\n",
            "179 episodes - episode_reward: 55.508 [8.000, 155.000] - loss: 1.029 - mae: 14.423 - mean_q: 29.281\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 76s 8ms/step - reward: 1.0000\n",
            "73 episodes - episode_reward: 136.384 [108.000, 172.000] - loss: 1.471 - mae: 26.463 - mean_q: 53.606\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 77s 8ms/step - reward: 1.0000\n",
            "71 episodes - episode_reward: 142.296 [82.000, 241.000] - loss: 0.999 - mae: 27.570 - mean_q: 55.606\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 77s 8ms/step - reward: 1.0000\n",
            "58 episodes - episode_reward: 170.397 [116.000, 281.000] - loss: 0.833 - mae: 29.640 - mean_q: 59.828\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 77s 8ms/step - reward: 1.0000\n",
            "done, took 385.304 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f38d01e0a90>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see below the DQN performed significantly better in comparison yielding high rewards at very low episode numbers. This is because in Q learning only 1 value was getting updated as opposed to the entire matrix in case of DQN. This results in better performance at lesser number of episodes"
      ],
      "metadata": {
        "id": "gwKWECVW4zCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = dqn.test(env, nb_episodes=10, visualize=False)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSnVtAX5ZQSw",
        "outputId": "c8f5ce0d-fa19-4fd7-d1d8-a04fa5a12b02"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 180.000, steps: 180\n",
            "Episode 2: reward: 158.000, steps: 158\n",
            "Episode 3: reward: 164.000, steps: 164\n",
            "Episode 4: reward: 165.000, steps: 165\n",
            "Episode 5: reward: 162.000, steps: 162\n",
            "Episode 6: reward: 173.000, steps: 173\n",
            "Episode 7: reward: 176.000, steps: 176\n",
            "Episode 8: reward: 177.000, steps: 177\n",
            "Episode 9: reward: 173.000, steps: 173\n",
            "Episode 10: reward: 180.000, steps: 180\n",
            "170.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = build_model(states,actions)"
      ],
      "metadata": {
        "id": "3Mo8MOILZnDF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_1 = build_agent(model_1,actions, policy = BoltzmannQPolicy())\n",
        "dqn_1.compile(Adam(learning_rate = 1e-3), metrics = ['mae'])\n",
        "dqn_1.fit(env, nb_steps=50000, visualize=False, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_jXPKYCZuw_",
        "outputId": "3dbe51aa-1a8f-4f94-8fbe-2d8b6873e0de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r    1/10000 [..............................] - ETA: 50:01 - reward: 1.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 76s 8ms/step - reward: 1.0000\n",
            "91 episodes - episode_reward: 108.044 [10.000, 400.000] - loss: 2.071 - mae: 19.197 - mean_q: 38.907\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 76s 8ms/step - reward: 1.0000\n",
            "44 episodes - episode_reward: 227.614 [158.000, 312.000] - loss: 3.325 - mae: 40.677 - mean_q: 82.265\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 78s 8ms/step - reward: 1.0000\n",
            "40 episodes - episode_reward: 248.550 [178.000, 500.000] - loss: 2.461 - mae: 43.881 - mean_q: 88.482\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 78s 8ms/step - reward: 1.0000\n",
            "43 episodes - episode_reward: 235.674 [166.000, 325.000] - loss: 1.625 - mae: 41.662 - mean_q: 83.842\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 78s 8ms/step - reward: 1.0000\n",
            "done, took 385.418 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5320576590>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Boltzmann Q-policy DQN performed quite well giving much higher rewards as opposed epsilon greedy Q method with rewards upto 500 in some episodes (which is the highest that can be achieved."
      ],
      "metadata": {
        "id": "EUN_A6Et5aDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores_1 = dqn_1.test(env, nb_episodes=10, visualize=False)\n",
        "print(np.mean(scores_1.history['episode_reward']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScFHiT-FmBTx",
        "outputId": "4d83482c-d0dc-44b9-ce4d-b1338fceca04"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 275.000, steps: 275\n",
            "Episode 2: reward: 240.000, steps: 240\n",
            "Episode 3: reward: 253.000, steps: 253\n",
            "Episode 4: reward: 256.000, steps: 256\n",
            "Episode 5: reward: 239.000, steps: 239\n",
            "Episode 6: reward: 307.000, steps: 307\n",
            "Episode 7: reward: 500.000, steps: 500\n",
            "Episode 8: reward: 500.000, steps: 500\n",
            "Episode 9: reward: 211.000, steps: 211\n",
            "Episode 10: reward: 500.000, steps: 500\n",
            "328.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = build_model(states,actions)\n",
        "dqn_2 = build_agent(model_2,actions, policy = GreedyQPolicy())\n",
        "dqn_2.compile(Adam(learning_rate = 1e-3), metrics = ['mae'])\n",
        "dqn_2.fit(env, nb_steps=50000, visualize=False, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGe6AoT3mQmH",
        "outputId": "07f468d9-d2f2-456d-c163-2d95b61b596b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r    1/10000 [..............................] - ETA: 50:43 - reward: 1.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 78s 8ms/step - reward: 1.0000\n",
            "326 episodes - episode_reward: 30.607 [8.000, 129.000] - loss: 1.022 - mae: 10.024 - mean_q: 20.765\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 76s 8ms/step - reward: 1.0000\n",
            "84 episodes - episode_reward: 118.048 [107.000, 135.000] - loss: 2.294 - mae: 24.835 - mean_q: 51.208\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 77s 8ms/step - reward: 1.0000\n",
            "82 episodes - episode_reward: 123.134 [109.000, 149.000] - loss: 1.284 - mae: 25.039 - mean_q: 51.244\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 78s 8ms/step - reward: 1.0000\n",
            "79 episodes - episode_reward: 126.658 [104.000, 157.000] - loss: 0.716 - mae: 23.954 - mean_q: 48.681\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 78s 8ms/step - reward: 1.0000\n",
            "done, took 387.630 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa3d0471290>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case of Greedy Q policy we achieved decent results. But the results were not as good the other 2 Q-learning model."
      ],
      "metadata": {
        "id": "H9mhTXqM52z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores_2 = dqn_2.test(env, nb_episodes=10, visualize=False)\n",
        "print(np.mean(scores_2.history['episode_reward']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t629UAKLpu11",
        "outputId": "9db6f23a-a6c6-4383-bc90-a5e0e60d7c11"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 117.000, steps: 117\n",
            "Episode 2: reward: 160.000, steps: 160\n",
            "Episode 3: reward: 122.000, steps: 122\n",
            "Episode 4: reward: 169.000, steps: 169\n",
            "Episode 5: reward: 138.000, steps: 138\n",
            "Episode 6: reward: 128.000, steps: 128\n",
            "Episode 7: reward: 122.000, steps: 122\n",
            "Episode 8: reward: 180.000, steps: 180\n",
            "Episode 9: reward: 141.000, steps: 141\n",
            "Episode 10: reward: 125.000, steps: 125\n",
            "140.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size = 4>**Reference**</font>\n",
        "\n",
        "\n",
        "\n",
        "1.   https://medium.com/swlh/using-q-learning-for-openais-cartpole-v1-4a216ef237df\n",
        "2.   https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
        "\n",
        "3. https://www.youtube.com/watch?v=cO5g5qLrLSo&t=536s\n",
        "\n",
        "<font size = 4>**Additional Resources**</font>\n",
        "\n",
        "1. https://towardsdatascience.com/deep-q-learning-for-the-cartpole-44d761085c2f\n",
        "\n",
        "2. https://www.youtube.com/watch?v=mlTDpdh24qs"
      ],
      "metadata": {
        "id": "wDiVZOccyp3b"
      }
    }
  ]
}